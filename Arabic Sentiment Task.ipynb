{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing importing ASTD data into database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mona\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[0] Before Padding [1, 577, 404]\n",
      "Shape of X :  (1589, 40)\n",
      "X[0] After Padding :  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   1 577 404]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mona\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:125: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1271/1271 [==============================] - 31s 25ms/step - loss: 0.6773 - acc: 0.5728\n",
      "Epoch 2/20\n",
      "1271/1271 [==============================] - 28s 22ms/step - loss: 0.4406 - acc: 0.8977\n",
      "Epoch 3/20\n",
      "1271/1271 [==============================] - 27s 21ms/step - loss: 0.2572 - acc: 0.9583\n",
      "Epoch 4/20\n",
      "1271/1271 [==============================] - 27s 21ms/step - loss: 0.1160 - acc: 0.9898\n",
      "Epoch 5/20\n",
      "1271/1271 [==============================] - 27s 21ms/step - loss: 0.0463 - acc: 0.9992\n",
      "Epoch 6/20\n",
      "1271/1271 [==============================] - 27s 21ms/step - loss: 0.0262 - acc: 0.9976\n",
      "Epoch 7/20\n",
      "1271/1271 [==============================] - 27s 22ms/step - loss: 0.0145 - acc: 0.9992\n",
      "Epoch 8/20\n",
      "1271/1271 [==============================] - 28s 22ms/step - loss: 0.0088 - acc: 0.9992\n",
      "Epoch 9/20\n",
      "1271/1271 [==============================] - 29s 22ms/step - loss: 0.0072 - acc: 0.9992\n",
      "Epoch 10/20\n",
      "1271/1271 [==============================] - 28s 22ms/step - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 11/20\n",
      "1271/1271 [==============================] - 27s 21ms/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 12/20\n",
      "1271/1271 [==============================] - 27s 21ms/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 13/20\n",
      "1271/1271 [==============================] - 27s 22ms/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "1271/1271 [==============================] - 29s 23ms/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "1271/1271 [==============================] - 27s 22ms/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "1271/1271 [==============================] - 27s 22ms/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 17/20\n",
      "1271/1271 [==============================] - 27s 22ms/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "1271/1271 [==============================] - 27s 21ms/step - loss: 8.5841e-04 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "1271/1271 [==============================] - 27s 22ms/step - loss: 7.3468e-04 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "1271/1271 [==============================] - 28s 22ms/step - loss: 6.4543e-04 - acc: 1.0000\n",
      "Y_prediction: \n",
      " [1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0]\n",
      "Y_test: \n",
      "  ['1', '1', '0', '1', '1', '0', '0', '1', '1', '1', '1', '1', '1', '0', '0', '1', '0', '0', '0', '0', '1', '1', '0', '1', '0', '1', '0', '0', '1', '0', '1', '1', '1', '0', '1', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '1', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '1', '0', '1', '0', '0', '1', '0', '0', '0', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '1', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '0', '0', '1', '1', '1', '0', '0', '0', '0', '0', '1', '1', '0', '0', '0', '0', '1', '0', '0', '1', '1', '0', '0', '1', '0', '0', '1', '0', '0', '0', '0', '1', '1', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '1', '1', '0', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '1', '0', '1', '1', '0', '1', '1', '1', '0', '0', '1', '1', '1', '1', '1', '0', '1', '0', '0', '1', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '1', '0', '1', '1', '0', '0', '1', '0', '0', '1', '0', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '0', '1', '1', '1', '1', '1', '1', '0', '0', '1', '1', '0', '1', '0', '0', '0', '0', '0', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '1', '1', '0', '0', '1', '0', '0', '1', '1', '1', '0', '1', '0', '0', '1', '0', '1', '0', '0', '1', '1', '0', '1', '1', '1', '0', '0', '1', '1', '0', '0', '1', '1', '1', '1', '1', '0', '0', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '0', '1', '1', '1', '0', '0', '0', '0', '0', '1', '1', '1', '0', '1', '1', '0', '0', '0', '0', '1', '1', '0', '1', '0', '1', '1', '1', '0', '0', '1', '1', '0']\n",
      "\n",
      " Accuracy of model is \n",
      " =================== \n",
      " 0.7138364764879335 \n",
      " ===================\n",
      "\n",
      " Finishing importing result in database\n"
     ]
    }
   ],
   "source": [
    "import gensim.models.keyedvectors as word2vec \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve,  roc_auc_score, classification_report\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import cx_Oracle\n",
    "import pandas as pd \n",
    "from sqlalchemy import create_engine\n",
    "import csv \n",
    "\n",
    "\n",
    "\n",
    "insertQuery=\" INSERT INTO SocialData.ASTD VALUES(:1,:2)\"\n",
    "dsn_tns = cx_Oracle.makedsn('localhost', '1521', service_name='xe') \n",
    "conn = cx_Oracle.connect(user=r'SocialData', password='SocialData',dsn=dsn_tns,encoding = \"UTF-8\", nencoding = \"UTF-8\") \n",
    "c=conn.cursor()\n",
    "\n",
    "#import into database ASTD.csv data\n",
    "\n",
    "with open(u'ASTD.csv',encoding=\"utf-8-sig\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "  #  next(reader)\n",
    "    for row in reader:\n",
    "     \n",
    "        c.execute(insertQuery ,(row[0],row[1]))\n",
    "        conn.commit()   \n",
    "        #print(row[1])\n",
    "\n",
    "print(\"Finishing importing ASTD data into database\")\n",
    "\n",
    "dsn_tns1 = cx_Oracle.makedsn('localhost', '1521', service_name='xe') #if needed, place an 'r' before any parameter in order to address any special character such as '\\'.\n",
    "conn1= cx_Oracle.connect(user=r'SocialData', password='SocialData', dsn=dsn_tns1,encoding = \"UTF-8\", nencoding = \"UTF-8\") #if needed, place an 'r' before any parameter in order to address any special character such as '\\'. For example, if your user name contains '\\', you'll need to place 'r' before the user name: user=r'User Name'\n",
    "\n",
    "\n",
    "tweetsData = pd.read_sql('select * from SocialData.ASTD',conn1)\n",
    "tweetsData=tweetsData[1:]\n",
    "tweetsData = tweetsData.iloc[np.random.permutation(len(tweetsData))] #shuffling data\n",
    "tweetsData.head()\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname) s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#Set random seed\n",
    "#np.random.seed(24)\n",
    "\n",
    "'''\n",
    "Get Dataset\n",
    "'''\n",
    "#read CSV file containing tweets and labels, using Pandas , to get a dataframe\n",
    "#tweetsData = pd.read_csv('ASTD.csv') #skiping these two rows as they have some bad data\n",
    "\n",
    "\n",
    "\n",
    "#tweetsData.head()\n",
    "\n",
    "#Dividing the dataset into features and lables\n",
    "tweets = tweetsData['SENTENCE']\n",
    "train_df, test_df = train_test_split(tweets, train_size=0.2)\n",
    "test_df=list(test_df)\n",
    "#print(\"Test Data: \\n\",test_df)\n",
    "labels = tweetsData['SENTIMENT']\n",
    "\n",
    "#print(\"tweets\",tweets)\n",
    "\n",
    "\n",
    "#check the distribution of lebels\n",
    "\n",
    "#labels_count = labels.value_counts()\n",
    "#labels_count.plot(kind=\"bar\")\n",
    "#print(labels.value_counts())\n",
    "\n",
    "#Looks like the distribution is even\n",
    "\n",
    "#Lower and split the dialog\n",
    "#and use regular expression to keep only letters we will use nltk Regular expression package\n",
    "#tkr = RegexpTokenizer('[a-zA-Z@]+')\n",
    "\n",
    "tweets_split = []\n",
    "\n",
    "for i, line in enumerate(tweets):\n",
    "    #print(line)\n",
    "    tweet = wordpunct_tokenize(line) \n",
    "    tweets_split.append(tweet)\n",
    "\n",
    "#print(\"tweets_split\",tweets_split[0])\n",
    "\n",
    "'''\n",
    "Use pretrained Word2Vec model \n",
    "'''\n",
    "\n",
    "w2vModel = word2vec.KeyedVectors.load_word2vec_format('arabic-news.bin', binary=True)\n",
    "\n",
    "#Convert words to integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tweets_split)\n",
    "X = tokenizer.texts_to_sequences(tweets_split)\n",
    "print(\"X[0] Before Padding\",X[0])\n",
    "\n",
    "\n",
    "#lenght of tweet to consider\n",
    "maxlentweet = 40\n",
    "#add padding\n",
    "X = pad_sequences(X, maxlen=maxlentweet)\n",
    "#print(\"X After Padding:\\n\",X)\n",
    "print(\"Shape of X : \" ,X.shape)\n",
    "print(\"X[0] After Padding : \",X[0])\n",
    "\n",
    "                              \n",
    "#create a embedding layer using Google pre triained word2vec (50000 words)\n",
    "embedding_layer = Embedding(input_dim=w2vModel.syn0.shape[0], output_dim=w2vModel.syn0.shape[1], weights=[w2vModel.syn0], \n",
    "                            input_length=X.shape[1])\n",
    "\n",
    "#create model\n",
    "\n",
    "lstm_out = 80\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(units=lstm_out))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#print(\"Model Summary: \\n\",model.summary())\n",
    "\n",
    "\n",
    "\n",
    "#split dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, labels, test_size= 0.2)\n",
    "\n",
    "#fit model\n",
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs=20, batch_size=batch_size)\n",
    "\n",
    "#analyze the results\n",
    "score, acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size=batch_size)\n",
    "y_pred = model.predict_classes(X_test)\n",
    "\n",
    "#print(\"y_pred \",y_pred)\n",
    "\n",
    "#print(\"score\",score)\n",
    "\n",
    "#adding results to data base \n",
    "ResultSaving=\" INSERT INTO SocialData.PREDECTIONS  VALUES(:1,:2,:3)\"\n",
    "y_pred2=list()\n",
    "\n",
    "y_pred2 = [item for sublist in y_pred for item in sublist]\n",
    "print(\"Y_prediction: \\n\" , y_pred2)\n",
    "\n",
    "#print(\"Tweet: \",tweets[0])\n",
    "#print(\"y_pred: \",y_pred[0][0])\n",
    "#print(\"y_pred: \",Y_test[0][0])\n",
    "Y_test=list(Y_test)\n",
    "print(\"Y_test: \\n \",Y_test)\n",
    "\n",
    "\n",
    "for i in range (0,len(y_pred2)-1):\n",
    "    c.execute(ResultSaving,(str(test_df[i]),str(y_pred2[i]),str(Y_test[i])))\n",
    "    conn.commit()\n",
    "        \n",
    "print(\"\\n Accuracy of model is \\n =================== \\n\",acc,\"\\n ===================\")\n",
    "\n",
    "print(\"\\n Finishing importing result in database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
